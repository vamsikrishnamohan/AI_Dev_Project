{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11563022,"sourceType":"datasetVersion","datasetId":7250052},{"sourceId":11571455,"sourceType":"datasetVersion","datasetId":7254592}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport json\nfrom tqdm import tqdm\nimport re\nimport time\nimport argparse","metadata":{"_uuid":"7759b3bc-8050-4539-96b9-60c6903995e4","_cell_guid":"17a16fb5-f86c-4ff3-8932-b68854725d90","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-04-26T07:38:45.302373Z","iopub.execute_input":"2025-04-26T07:38:45.303011Z","iopub.status.idle":"2025-04-26T07:38:45.307389Z","shell.execute_reply.started":"2025-04-26T07:38:45.302984Z","shell.execute_reply":"2025-04-26T07:38:45.306837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:19.503948Z","iopub.execute_input":"2025-04-26T03:44:19.504132Z","iopub.status.idle":"2025-04-26T03:44:27.21742Z","shell.execute_reply.started":"2025-04-26T03:44:19.504116Z","shell.execute_reply":"2025-04-26T03:44:27.216857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_path = \"/kaggle/input/crohme-data-basic/filtered_basic_arithmetic/train\"\nimg_path = train_path+\"/images/expr_\"+\"009203.png\"\nprint(img_path)\nimg = Image.open(img_path)\nplt.imshow(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.218753Z","iopub.execute_input":"2025-04-26T03:44:27.219082Z","iopub.status.idle":"2025-04-26T03:44:27.485351Z","shell.execute_reply.started":"2025-04-26T03:44:27.219063Z","shell.execute_reply":"2025-04-26T03:44:27.484519Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_path = \"/kaggle/input/crohme-data/val\"\n#print(os.path.join(train_path,\"labels\"))\ncount = 0\nfor files in os.listdir(os.path.join(train_path,\"labels\"))[:10]:\n    #print(files)\n    label_path = train_path+\"/labels/\"+files\n    count+=1\n    #print(label_path)\n    f = open(label_path, 'r')\n    print(f.read())\nprint(count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.486222Z","iopub.execute_input":"2025-04-26T03:44:27.486517Z","iopub.status.idle":"2025-04-26T03:44:27.551997Z","shell.execute_reply.started":"2025-04-26T03:44:27.486491Z","shell.execute_reply":"2025-04-26T03:44:27.551488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.55271Z","iopub.execute_input":"2025-04-26T03:44:27.55311Z","iopub.status.idle":"2025-04-26T03:44:27.556788Z","shell.execute_reply.started":"2025-04-26T03:44:27.553086Z","shell.execute_reply":"2025-04-26T03:44:27.556152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration\nclass Config:\n    def __init__(self):\n        # Dataset paths\n        self.data_root = '/kaggle/input/crohme-data'\n        \n        # Model parameters\n        self.embed_dim = 256\n        self.hidden_dim = 512\n        self.num_layers = 1\n        self.dropout = 0.3\n        self.max_seq_len = 150\n        \n        # Training parameters\n        self.batch_size = 32\n        self.num_epochs = 10\n        self.learning_rate = 0.001\n        self.teacher_forcing_ratio = 0.9\n        self.teacher_forcing_decay = 0.9\n        self.grad_clip = 5.0\n        \n        # Tokenizer parameters\n        self.special_tokens = {\n            'PAD': '<PAD>',\n            'START': '<START>',\n            'END': '<END>',\n            'UNK': '<UNK>'\n        }\n        \n        # Image preprocessing\n        self.img_height = 128\n        self.img_width = 512\n        \n        # Checkpoint parameters\n        self.checkpoint_dir = 'checkpoints'\n        self.log_dir = 'logs'\n        \n        # Device\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.55737Z","iopub.execute_input":"2025-04-26T03:44:27.557543Z","iopub.status.idle":"2025-04-26T03:44:27.574393Z","shell.execute_reply.started":"2025-04-26T03:44:27.557528Z","shell.execute_reply":"2025-04-26T03:44:27.573727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenizer for LaTeX expressions\nclass LaTeXTokenizer:\n    def __init__(self, config):\n        self.config = config\n        self.token2idx = {}\n        self.idx2token = {}\n        self.build_vocab([])  # Initialize with special tokens\n        \n    def build_vocab(self, latex_expressions):\n        # Add special tokens\n        vocab = [self.config.special_tokens[token] for token in ['PAD', 'START', 'END', 'UNK']]\n        \n        # Add all unique tokens from latex expressions\n        all_tokens = []\n        for expr in latex_expressions:\n            tokens = self._tokenize(expr)\n            all_tokens.extend(tokens)\n        \n        # Count token frequencies\n        token_counter = Counter(all_tokens)\n        tokens = [token for token, _ in token_counter.most_common()]\n        \n        # Add tokens to vocabulary that aren't already special tokens\n        for token in tokens:\n            if token not in vocab:\n                vocab.append(token)\n        \n        # Create mappings\n        self.token2idx = {token: idx for idx, token in enumerate(vocab)}\n        self.idx2token = {idx: token for idx, token in enumerate(vocab)}\n        \n        return self\n    \n    def _tokenize(self, latex_str):\n        \"\"\"\n        Tokenize a LaTeX string.\n        This is a simplified approach - in a production system, you might need \n        more sophisticated tokenization based on LaTeX syntax.\n        \"\"\"\n        # Remove extra whitespace\n        latex_str = latex_str.strip()\n        \n        # Handle special LaTeX commands\n        pattern = r'(\\\\[a-zA-Z]+|[^a-zA-Z0-9\\s])'\n        \n        # Split by the pattern but keep the delimiters\n        parts = re.split(f'({pattern})', latex_str)\n        \n        # Filter out empty strings and strip whitespace\n        tokens = [part.strip() for part in parts if part.strip()]\n        \n        return tokens\n    \n    def encode(self, latex_str):\n        \"\"\"Convert LaTeX string to token IDs\"\"\"\n        tokens = self._tokenize(latex_str)\n        \n        # Add START and END tokens\n        tokens = [self.config.special_tokens['START']] + tokens + [self.config.special_tokens['END']]\n        \n        # Convert to indices, using UNK for unknown tokens\n        unk_idx = self.token2idx[self.config.special_tokens['UNK']]\n        indices = [self.token2idx.get(token, unk_idx) for token in tokens]\n        \n        return indices\n    \n    def decode(self, indices):\n        \"\"\"Convert token IDs back to LaTeX string\"\"\"\n        # Convert indices to tokens\n        start_idx = self.token2idx[self.config.special_tokens['START']]\n        end_idx = self.token2idx[self.config.special_tokens['END']]\n        pad_idx = self.token2idx[self.config.special_tokens['PAD']]\n        \n        # Filter out special tokens\n        tokens = []\n        for idx in indices:\n            if idx == end_idx:  # Stop at END token\n                break\n            if idx not in [start_idx, pad_idx]:  # Skip START and PAD tokens\n                tokens.append(self.idx2token[idx])\n        \n        # Join tokens (with space between symbols for readability)\n        latex = ' '.join(tokens)\n        \n        # Clean up spaces around certain symbols\n        latex = re.sub(r'\\s+', ' ', latex)  # Replace multiple spaces with single space\n        for symbol in ['+', '-', '=', '>', '<', '\\\\leq', '\\\\geq']:\n            latex = latex.replace(f' {symbol} ', f' {symbol} ')\n        \n        return latex.strip()\n    \n    @property\n    def vocab_size(self):\n        return len(self.token2idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.5751Z","iopub.execute_input":"2025-04-26T03:44:27.575324Z","iopub.status.idle":"2025-04-26T03:44:27.595264Z","shell.execute_reply.started":"2025-04-26T03:44:27.575309Z","shell.execute_reply":"2025-04-26T03:44:27.594667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset class for CROHME\nclass CROHMEDataset(Dataset):\n    def __init__(self, data_dir, tokenizer, config, split='train', transform=None):\n        self.data_dir = data_dir\n        self.split = split\n        self.tokenizer = tokenizer\n        self.config = config\n        self.transform = transform if transform else self._get_default_transform()\n        \n        # Get all image paths\n        split_dir = os.path.join(data_dir, split)\n        self.image_paths = sorted(glob.glob(os.path.join(split_dir, 'images', '*.png')))\n        \n        # Load all LaTeX expressions\n        self.latex_expressions = []\n        for img_path in self.image_paths:\n            # Get corresponding label path\n            file_id = os.path.basename(img_path).split('.')[0]\n            label_path = os.path.join(split_dir, 'labels', f\"{file_id}.txt\")\n            \n            if os.path.exists(label_path):\n                with open(label_path, 'r', encoding='utf-8') as f:\n                    latex = f.read().strip()\n                self.latex_expressions.append(latex)\n            else:\n                print(f\"Warning: Label not found for {img_path}\")\n                self.latex_expressions.append(\"\")  # Empty placeholder\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load and transform image\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('L')  # Convert to grayscale\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        # Get and encode LaTeX\n        latex = self.latex_expressions[idx]\n        encoded_latex = self.tokenizer.encode(latex)\n        \n        # Pad sequence if needed\n        if len(encoded_latex) > self.config.max_seq_len:\n            encoded_latex = encoded_latex[:self.config.max_seq_len]\n        else:\n            pad_idx = self.tokenizer.token2idx[self.config.special_tokens['PAD']]\n            encoded_latex = encoded_latex + [pad_idx] * (self.config.max_seq_len - len(encoded_latex))\n        \n        return {\n            'image': image,\n            'latex_tokens': torch.tensor(encoded_latex, dtype=torch.long),\n            'latex_str': latex,\n            'image_path': img_path\n        }\n    \n    def _get_default_transform(self):\n        return transforms.Compose([\n            transforms.Resize((self.config.img_height, self.config.img_width)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5], std=[0.5])\n        ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.595961Z","iopub.execute_input":"2025-04-26T03:44:27.59623Z","iopub.status.idle":"2025-04-26T03:44:27.615605Z","shell.execute_reply.started":"2025-04-26T03:44:27.596182Z","shell.execute_reply":"2025-04-26T03:44:27.61496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Attention module\nclass AttentionModule(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim):\n        super().__init__()\n        self.encoder_dim = encoder_dim\n        self.decoder_dim = decoder_dim\n        \n        # Attention layers\n        self.attn = nn.Linear(encoder_dim + decoder_dim, decoder_dim)\n        self.v = nn.Linear(decoder_dim, 1, bias=False)\n        \n    def forward(self, encoder_features, decoder_hidden):\n        \"\"\"\n        encoder_features: (batch_size, feature_size, height, width)\n        decoder_hidden: (batch_size, decoder_dim)\n        \"\"\"\n        batch_size = encoder_features.size(0)\n        \n        # Reshape encoder features\n        feature_size = encoder_features.size(1)\n        num_pixels = encoder_features.size(2) * encoder_features.size(3)\n        \n        encoder_features = encoder_features.permute(0, 2, 3, 1).contiguous()\n        encoder_features = encoder_features.view(batch_size, num_pixels, feature_size)\n        \n        # Repeat decoder hidden state\n        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, num_pixels, 1)\n        \n        # Calculate attention weights\n        attention_input = torch.cat((decoder_hidden, encoder_features), dim=2)\n        attention = torch.tanh(self.attn(attention_input))\n        attention = self.v(attention).squeeze(2)\n        \n        # Apply softmax to get attention weights\n        alpha = F.softmax(attention, dim=1)\n        alpha = alpha.unsqueeze(2)\n        \n        # Apply attention weights to encoder features\n        context_vector = (encoder_features * alpha).sum(dim=1)\n        \n        return context_vector, alpha","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.617893Z","iopub.execute_input":"2025-04-26T03:44:27.618061Z","iopub.status.idle":"2025-04-26T03:44:27.632829Z","shell.execute_reply.started":"2025-04-26T03:44:27.618048Z","shell.execute_reply":"2025-04-26T03:44:27.632152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, encoded_dim):\n        super().__init__()\n        \n        # Use ResNet18 as backbone\n        resnet = models.resnet18(pretrained=True)\n        \n        # Remove final fully connected layer and pooling\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        \n        # Adjust first conv layer to accept grayscale images\n        self.resnet[0] = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        \n        # Add a projection layer to get the desired dimension\n        self.projection = nn.Conv2d(512, encoded_dim, kernel_size=1)\n        \n    def forward(self, images):\n        features = self.resnet(images)\n        features = self.projection(features)\n        return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.633626Z","iopub.execute_input":"2025-04-26T03:44:27.633853Z","iopub.status.idle":"2025-04-26T03:44:27.651552Z","shell.execute_reply.started":"2025-04-26T03:44:27.633838Z","shell.execute_reply":"2025-04-26T03:44:27.650856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decoder - LSTM with attention\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, encoder_dim, num_layers=1, dropout=0.5):\n        super().__init__()\n        \n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.hidden_dim = hidden_dim\n        self.encoder_dim = encoder_dim\n        self.num_layers = num_layers\n        \n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        \n        # Attention module\n        self.attention = AttentionModule(encoder_dim, hidden_dim)\n        \n        # LSTM layer\n        self.lstm = nn.LSTM(\n            input_size=embed_dim + encoder_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        \n        # Output projection\n        self.output = nn.Linear(hidden_dim, vocab_size)\n        \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward_step(self, encoder_features, prev_token, hidden=None):\n        \"\"\"Single step forward\"\"\"\n        # Get embeddings\n        embed = self.embedding(prev_token)  # (batch_size, 1, embed_dim)\n        \n        batch_size = prev_token.size(0)\n        \n        # Initialize hidden state if None\n        if hidden is None:\n            # Initialize hidden state and cell state as zeros\n            h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(encoder_features.device)\n            c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(encoder_features.device)\n            hidden = (h_0, c_0)\n        \n        # Extract hidden state (ignore cell state for attention)\n        hidden_state = hidden[0][-1]  # Last layer's hidden state\n        \n        # Rest of the method remains the same...\n        # Apply attention\n        context, _ = self.attention(encoder_features, hidden_state)\n        context = context.unsqueeze(1)  # (batch_size, 1, encoder_dim)\n        \n        # Concatenate embedding and context vector\n        lstm_input = torch.cat([embed, context], dim=2)\n        \n        # LSTM step\n        output, hidden = self.lstm(lstm_input, hidden)\n    \n        # Project to vocabulary space\n        output = self.output(self.dropout(output))\n        \n        return output, hidden\n    \n    def forward(self, encoder_features, targets=None, teacher_forcing_ratio=0.5, max_len=None):\n        \"\"\"\n        Forward pass with optional teacher forcing\n        encoder_features: (batch_size, encoder_dim, height, width)\n        targets: (batch_size, max_len) - token indices\n        \"\"\"\n        batch_size = encoder_features.size(0)\n        \n        # Initialize hidden state\n        # Create hidden state and cell state with appropriate dimensions\n        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(encoder_features.device)\n        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(encoder_features.device)\n        hidden = (h_0, c_0)\n        \n        # Determine sequence length\n        if targets is not None:\n            max_len = targets.size(1)\n        \n        # Tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, max_len, self.vocab_size).to(encoder_features.device)\n        \n        # First input is always <START> token\n        start_idx = targets[:, 0] if targets is not None else torch.ones(batch_size).long().to(encoder_features.device)\n        input_token = start_idx.unsqueeze(1)  # (batch_size, 1)\n        \n        # Generate sequence\n        for t in range(max_len):\n            # Forward step\n            output, hidden = self.forward_step(encoder_features, input_token, hidden)\n            \n            # Store output\n            outputs[:, t:t+1, :] = output\n            \n            # Determine next input token\n            use_teacher_forcing = (random.random() < teacher_forcing_ratio) and targets is not None\n            \n            if use_teacher_forcing and t < max_len - 1:\n                # Use ground truth as next input\n                input_token = targets[:, t+1:t+2]\n            else:\n                # Use model's prediction as next input\n                _, top_indices = output.topk(1, dim=2)\n                input_token = top_indices.squeeze(2)\n        \n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.652228Z","iopub.execute_input":"2025-04-26T03:44:27.652435Z","iopub.status.idle":"2025-04-26T03:44:27.670453Z","shell.execute_reply.started":"2025-04-26T03:44:27.65242Z","shell.execute_reply":"2025-04-26T03:44:27.669879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Complete model combining encoder and decoder\nclass HandwrittenMathRecognizer(nn.Module):\n    def __init__(self, config, vocab_size):\n        super().__init__()\n        self.config = config\n        \n        # Encoder and decoder\n        self.encoder = Encoder(encoded_dim=config.hidden_dim)\n        self.decoder = Decoder(\n            vocab_size=vocab_size,\n            embed_dim=config.embed_dim,\n            hidden_dim=config.hidden_dim,\n            encoder_dim=config.hidden_dim,\n            num_layers=config.num_layers,\n            dropout=config.dropout\n        )\n        \n    def forward(self, images, targets=None, teacher_forcing_ratio=0.5):\n        # Encode images\n        encoder_features = self.encoder(images)\n        \n        # Decode with or without teacher forcing\n        outputs = self.decoder(\n            encoder_features, \n            targets, \n            teacher_forcing_ratio, \n            max_len=self.config.max_seq_len if targets is None else None\n        )\n        \n        return outputs\n    \n    def generate(self, images):\n        \"\"\"Generate LaTeX expressions without teacher forcing\"\"\"\n        with torch.no_grad():\n            encoder_features = self.encoder(images)\n            outputs = self.decoder(\n                encoder_features, \n                targets=None, \n                teacher_forcing_ratio=0.0, \n                max_len=self.config.max_seq_len\n            )\n            \n            # Get predicted tokens\n            _, predicted = outputs.max(2)\n            \n            return predicted","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.67141Z","iopub.execute_input":"2025-04-26T03:44:27.671634Z","iopub.status.idle":"2025-04-26T03:44:27.690914Z","shell.execute_reply.started":"2025-04-26T03:44:27.671619Z","shell.execute_reply":"2025-04-26T03:44:27.69024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_latex(latex_string):\n    \"\"\"Clean LaTeX expressions by removing unwanted duplicate symbols\"\"\"\n    # List of symbols that shouldn't be repeated\n    no_repeat_symbols = [\n        '+', '=', '^', '-', '*', '/', '\\\\times', '\\\\div', '_', '.',\n        '>', '<', '!', '\\\\rightarrow', '\\\\leftarrow', '\\\\Rightarrow', '\\\\Leftarrow',\n        '\\\\leq', '\\\\geq', '\\\\approx', '\\\\sim', '\\\\cong', '\\\\neq',\n        '(', ')', '[', ']', '{', '}', '|', '\\\\|', '\\\\langle', '\\\\rangle'\n    ]\n    \n    # Create regex pattern for these symbols\n    pattern = '|'.join([f'({re.escape(sym)}\\\\s*{re.escape(sym)})' for sym in no_repeat_symbols])\n    \n    # Replace duplicates with single occurrences\n    cleaned = re.sub(pattern, lambda m: m.group(0)[0], latex_string)\n    \n    # Additional cleaning for spacing issues\n    cleaned = re.sub(r'\\s+', '', cleaned)  # Normalize spaces\n    cleaned = cleaned.strip()\n    \n    return cleaned","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T08:16:53.83175Z","iopub.execute_input":"2025-04-26T08:16:53.832557Z","iopub.status.idle":"2025-04-26T08:16:53.837622Z","shell.execute_reply.started":"2025-04-26T08:16:53.83253Z","shell.execute_reply":"2025-04-26T08:16:53.836903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef calculate_metrics(predictions, targets, tokenizer):\n    \"\"\"Calculate evaluation metrics\"\"\"\n    # Convert token indices to LaTeX strings\n    pred_latex = [tokenizer.decode(pred.tolist()) for pred in predictions]\n    target_latex = [tokenizer.decode(target.tolist()) for target in targets]\n    \n    # Clean LaTeX expressions\n    pred_latex_clean = [clean_latex(latex) for latex in pred_latex]\n    target_latex_clean = [clean_latex(latex) for latex in target_latex]\n    \n    # Calculate exact match accuracy with cleaned expressions\n    exact_matches = sum(pred == target for pred, target in zip(pred_latex_clean, target_latex_clean))\n    exact_match_accuracy = exact_matches / len(pred_latex) if len(pred_latex) > 0 else 0\n    \n    # Calculate token accuracy\n    total_tokens = 0\n    correct_tokens = 0\n    \n    for pred, target in zip(predictions, targets):\n        # Find end index (based on END token or max sequence length)\n        end_idx = tokenizer.config.max_seq_len\n        for i, token in enumerate(target):\n            if token.item() == tokenizer.token2idx[tokenizer.config.special_tokens['END']]:\n                end_idx = i + 1\n                break\n        \n        # Count correct tokens up to end index\n        min_len = min(len(pred), end_idx)\n        total_tokens += end_idx\n        correct_tokens += (pred[:min_len] == target[:min_len]).sum().item()\n    \n    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n    \n    # Calculate BLEU score\n    smoother = SmoothingFunction().method1\n    bleu_scores = []\n    \n    for pred, target in zip(pred_latex_clean, target_latex_clean):\n        # Convert strings to lists of characters for character-level BLEU\n        pred_chars = list(pred)\n        target_chars = list(target)\n        \n        # Calculate BLEU score with smoothing\n        try:\n            score = sentence_bleu([target_chars], pred_chars, smoothing_function=smoother)\n            bleu_scores.append(score)\n        except:\n            bleu_scores.append(0.0)\n    \n    # Average BLEU score\n    bleu_score = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n    \n    # Generate 5 random indices without replacement\n    total_samples = len(pred_latex)\n    random_indices = np.random.choice(total_samples, size=min(5, total_samples), replace=False)\n    \n    return {\n        'exact_match': exact_match_accuracy,\n        'token_accuracy': token_accuracy,\n        'bleu': bleu_score,\n        'pred_examples': [pred_latex_clean[i] for i in random_indices],  # Random sample predictions\n        'target_examples': [target_latex_clean[i] for i in random_indices]  # Corresponding targets\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T07:50:51.11033Z","iopub.execute_input":"2025-04-26T07:50:51.110892Z","iopub.status.idle":"2025-04-26T07:50:51.11979Z","shell.execute_reply.started":"2025-04-26T07:50:51.11087Z","shell.execute_reply":"2025-04-26T07:50:51.119237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_epoch(model, dataloader, criterion, optimizer, config, epoch):\n    model.train()\n    total_loss = 0\n    all_predictions = []\n    all_targets = []\n    \n    # Teacher forcing ratio with decay\n    teacher_forcing_ratio = config.teacher_forcing_ratio * (config.teacher_forcing_decay ** epoch)\n    \n    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1} Training\")\n    for batch in progress_bar:\n        images = batch['image'].to(config.device)\n        targets = batch['latex_tokens'].to(config.device)\n        \n        # Forward pass\n        outputs = model(images, targets, teacher_forcing_ratio)\n        \n        # Flatten for loss\n        outputs_flat = outputs.contiguous().view(-1, outputs.size(-1))\n        targets_flat = targets.contiguous().view(-1)\n        \n        # Compute loss (ignore_index already set in criterion)\n        loss = criterion(outputs_flat, targets_flat)\n        \n        # Backward + optimize\n        optimizer.zero_grad()\n        loss.backward()\n        if config.grad_clip > 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", tf_ratio=f\"{teacher_forcing_ratio:.2f}\")\n        \n        # Store predictions and targets for metric calculation\n        with torch.no_grad():\n            predictions = model.generate(images)\n            all_predictions.extend(predictions.detach().cpu())\n            all_targets.extend(targets.detach().cpu())\n    \n    # Calculate train metrics\n    train_metrics = calculate_metrics(all_predictions, all_targets, tokenizer)\n    train_metrics['loss'] = total_loss / len(dataloader)\n    \n    return train_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T07:30:31.922035Z","iopub.execute_input":"2025-04-26T07:30:31.922299Z","iopub.status.idle":"2025-04-26T07:30:31.929066Z","shell.execute_reply.started":"2025-04-26T07:30:31.92228Z","shell.execute_reply":"2025-04-26T07:30:31.928382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion, tokenizer, config):\n    model.eval()\n    total_loss = 0\n    all_predictions = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            # Get batch data\n            images = batch['image'].to(config.device)\n            targets = batch['latex_tokens'].to(config.device)\n            \n            # Forward pass for loss calculation\n            outputs = model(images, targets, teacher_forcing_ratio=0.0)\n            \n            # Reshape for loss calculation\n            outputs_flat = outputs.contiguous().view(-1, outputs.size(-1))\n            targets_flat = targets.contiguous().view(-1)\n            \n            # Calculate loss\n            loss = criterion(outputs_flat, targets_flat)\n            total_loss += loss.item()\n            \n            # Generate predictions for accuracy calculation\n            predictions = model.generate(images)\n            \n            # Store predictions and targets\n            all_predictions.extend(predictions.detach().cpu())\n            all_targets.extend(targets.detach().cpu())\n    \n    # Calculate metrics\n    metrics = calculate_metrics(all_predictions, all_targets, tokenizer)\n    metrics['loss'] = total_loss / len(dataloader)\n    \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T07:30:06.562716Z","iopub.execute_input":"2025-04-26T07:30:06.5634Z","iopub.status.idle":"2025-04-26T07:30:06.569208Z","shell.execute_reply.started":"2025-04-26T07:30:06.563375Z","shell.execute_reply":"2025-04-26T07:30:06.568577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create datasets\n#train_dataset = CROHMEDataset('/kaggle/input/crohme-data', tokenizer, config, split='train')\n#val_dataset = CROHMEDataset(config.data_root, tokenizer, config, split='val')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.753387Z","iopub.execute_input":"2025-04-26T03:44:27.753585Z","iopub.status.idle":"2025-04-26T03:44:27.772839Z","shell.execute_reply.started":"2025-04-26T03:44:27.75357Z","shell.execute_reply":"2025-04-26T03:44:27.772151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim.lr_scheduler import ReduceLROnPlateau\ndef train_model(config):\n    \"\"\"Main training function with metrics storage for plotting\"\"\"\n    print(f\"Using device: {config.device}\")\n    \n    # Create checkpoint directory if it doesn't exist\n    os.makedirs(config.checkpoint_dir, exist_ok=True)\n    os.makedirs(config.log_dir, exist_ok=True)\n    \n    # Initialize metrics storage for plotting\n    metrics_history = {\n        'train_loss': [],\n        'train_exact_match': [],\n        'train_token_accuracy': [],\n        'train_bleu': [],\n        'val_loss': [],\n        'val_exact_match': [],\n        'val_token_accuracy': [],\n        'val_bleu': [],\n        'epoch': []\n    }\n    \n    # Load dataset\n    print(\"Loading dataset...\")\n    \n    # First, get all LaTeX expressions to build vocabulary\n    all_latex = []\n    for split in ['train', 'val']:\n        split_dir = os.path.join(config.data_root, split)\n        label_files = glob.glob(os.path.join(split_dir, 'labels', '*.txt'))\n        \n        for label_file in tqdm(label_files, desc=f\"Reading {split} labels\"):\n            with open(label_file, 'r', encoding='utf-8') as f:\n                all_latex.append(f.read().strip())\n    \n    # Create tokenizer and build vocabulary\n    print(\"Building vocabulary...\")\n    tokenizer = LaTeXTokenizer(config)\n    tokenizer.build_vocab(all_latex)\n    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n    \n    # Create datasets\n    train_dataset = CROHMEDataset(config.data_root, tokenizer, config, split='train')\n    val_dataset = CROHMEDataset(config.data_root, tokenizer, config, split='val')\n    \n    print(f\"Train dataset size: {len(train_dataset)}\")\n    print(f\"Validation dataset size: {len(val_dataset)}\")\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=config.batch_size, \n        shuffle=True, \n        pin_memory=True,\n        num_workers=2\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=config.batch_size, \n        shuffle=False, \n        pin_memory=True,\n        num_workers=2\n    )\n    \n    # Create model\n    print(\"Creating model...\")\n    model = HandwrittenMathRecognizer(config, tokenizer.vocab_size).to(config.device)\n    \n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token2idx[config.special_tokens['PAD']])\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n    )\n    \n    # Training loop\n    print(f\"Starting training for {config.num_epochs} epochs...\")\n    best_val_loss = float('inf')\n    \n    for epoch in range(config.num_epochs):\n        # Train\n        train_metrics = train_epoch(model, train_loader, criterion, optimizer, config, epoch)\n        \n        # Evaluate\n        val_metrics = evaluate(model, val_loader, criterion, tokenizer, config)\n        val_loss = val_metrics['loss']\n        \n        # Update learning rate\n        scheduler.step(val_loss)\n        \n        # Store metrics for plotting\n        metrics_history['epoch'].append(epoch + 1)\n        metrics_history['train_loss'].append(train_metrics['loss'])\n        metrics_history['train_exact_match'].append(train_metrics['exact_match'])\n        metrics_history['train_token_accuracy'].append(train_metrics['token_accuracy']) \n        metrics_history['train_bleu'].append(train_metrics['bleu'])\n        metrics_history['val_loss'].append(val_metrics['loss'])\n        metrics_history['val_exact_match'].append(val_metrics['exact_match'])\n        metrics_history['val_token_accuracy'].append(val_metrics['token_accuracy'])\n        metrics_history['val_bleu'].append(val_metrics['bleu'])\n        \n        # Print metrics\n        print(f\"Epoch {epoch+1}/{config.num_epochs}:\")\n        print(f\"  Train Loss: {train_metrics['loss']:.4f}\")\n        print(f\"  Train Exact Match: {train_metrics['exact_match']:.4f}\")\n        print(f\"  Train Token Accuracy: {train_metrics['token_accuracy']:.4f}\")\n        print(f\"  Train BLEU Score: {train_metrics['bleu']:.4f}\")\n        print(f\"  Val Loss: {val_metrics['loss']:.4f}\")\n        print(f\"  Val Exact Match: {val_metrics['exact_match']:.4f}\")\n        print(f\"  Val Token Accuracy: {val_metrics['token_accuracy']:.4f}\")\n        print(f\"  Val BLEU Score: {val_metrics['bleu']:.4f}\")\n        \n        # Sample predictions\n        print(\"Sample predictions:\")\n        for i in range(min(3, len(val_metrics['pred_examples']))):\n            print(f\"  Pred: {val_metrics['pred_examples'][i]}\")\n            print(f\"  True: {val_metrics['target_examples'][i]}\")\n            print()\n        \n        # Save metrics history to JSON\n        metrics_path = os.path.join(config.log_dir, 'metrics_history.json')\n        with open(metrics_path, 'w') as f:\n            json.dump(metrics_history, f, indent=4)\n        \n        # Save checkpoint if improved\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            checkpoint_path = os.path.join(config.checkpoint_dir, 'best_model.pth')\n            \n            torch.save({\n                'epoch': epoch + 1,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n                'val_metrics': val_metrics,\n                'train_metrics': train_metrics,\n                'metrics_history': metrics_history,\n                'tokenizer': tokenizer,\n                'config': config\n            }, checkpoint_path)\n            \n            print(f\"Saved best model checkpoint to {checkpoint_path}\")\n        \n        # Always save latest model\n        checkpoint_path = os.path.join(config.checkpoint_dir, 'latest_model.pth')\n        torch.save({\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'val_metrics': val_metrics,\n            'train_metrics': train_metrics,\n            'metrics_history': metrics_history,\n            'tokenizer': tokenizer,\n            'config': config\n        }, checkpoint_path)\n    \n    print(\"Training complete!\")\n    \n    # Plot and save metrics graphs\n    plot_metrics(metrics_history, config.log_dir)\n    \n    return model, tokenizer, metrics_history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T07:36:36.661466Z","iopub.execute_input":"2025-04-26T07:36:36.661759Z","iopub.status.idle":"2025-04-26T07:36:36.676393Z","shell.execute_reply.started":"2025-04-26T07:36:36.661737Z","shell.execute_reply":"2025-04-26T07:36:36.675806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(model, tokenizer, config, test_loader=None):\n    \"\"\"Test the model on the test set\"\"\"\n    if test_loader is None:\n        # Create test dataset and loader\n        test_dataset = CROHMEDataset(config.data_root, tokenizer, config, split='test')\n        test_loader = DataLoader(\n            test_dataset, \n            batch_size=config.batch_size, \n            shuffle=False,\n            pin_memory=True\n        )\n    \n    # Evaluate\n    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token2idx[config.special_tokens['PAD']])\n    metrics = evaluate(model, test_loader, criterion, tokenizer, config)\n    \n    print(\"Test Results:\")\n    print(f\"  Loss: {metrics['loss']:.4f}\")\n    print(f\"  Exact Match: {metrics['exact_match']:.4f}\")\n    print(f\"  Token Accuracy: {metrics['token_accuracy']:.4f}\")\n    \n    # Sample predictions\n    print(\"Sample predictions:\")\n    for i in range(min(5, len(metrics['pred_examples']))):\n        print(f\"  Pred: {metrics['pred_examples'][i]}\")\n        print(f\"  True: {metrics['target_examples'][i]}\")\n        print()\n    \n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.791343Z","iopub.execute_input":"2025-04-26T03:44:27.791569Z","iopub.status.idle":"2025-04-26T03:44:27.810094Z","shell.execute_reply.started":"2025-04-26T03:44:27.791549Z","shell.execute_reply":"2025-04-26T03:44:27.809607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_image(model, tokenizer, image_path, config):\n    \"\"\"Predict LaTeX for a single image\"\"\"\n    # Load and preprocess image\n    transform = transforms.Compose([\n        transforms.Resize((config.img_height, config.img_width)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5], std=[0.5])\n    ])\n    \n    image = Image.open(image_path).convert('L')\n    image = transform(image).unsqueeze(0).to(config.device)\n    \n    # Generate prediction\n    model.eval()\n    with torch.no_grad():\n        prediction = model.generate(image)\n        latex = tokenizer.decode(prediction[0].tolist())\n    \n    return clean_latex(latex)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T08:14:10.105158Z","iopub.execute_input":"2025-04-26T08:14:10.105813Z","iopub.status.idle":"2025-04-26T08:14:10.11091Z","shell.execute_reply.started":"2025-04-26T08:14:10.10579Z","shell.execute_reply":"2025-04-26T08:14:10.110239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_checkpoint(checkpoint_path, device=None):\n    \"\"\"Load model from checkpoint\"\"\"\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    \n    # Get config and tokenizer\n    config = checkpoint['config']\n    tokenizer = checkpoint['tokenizer']\n    \n    # Create model\n    model = HandwrittenMathRecognizer(config, tokenizer.vocab_size).to(device)\n    \n    # Load weights\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    return model, tokenizer, config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T03:44:27.831726Z","iopub.execute_input":"2025-04-26T03:44:27.832101Z","iopub.status.idle":"2025-04-26T03:44:27.846841Z","shell.execute_reply.started":"2025-04-26T03:44:27.832086Z","shell.execute_reply":"2025-04-26T03:44:27.846112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_metrics(metrics_history, save_dir):\n    \"\"\"Plot training and validation metrics and save the figures\"\"\"\n    import matplotlib.pyplot as plt\n    \n    # Create metrics directory if it doesn't exist\n    metrics_dir = os.path.join(save_dir, 'metrics_plots')\n    os.makedirs(metrics_dir, exist_ok=True)\n    \n    # Set style\n    plt.style.use('ggplot')\n    \n    # Create subplots for each metric\n    metrics_to_plot = [\n        ('loss', 'Loss'),\n        ('exact_match', 'Exact Match Accuracy'),\n        ('token_accuracy', 'Token Accuracy'),\n        ('bleu', 'BLEU Score')\n    ]\n    \n    for metric_key, metric_title in metrics_to_plot:\n        plt.figure(figsize=(10, 6))\n        \n        # Plot training and validation metrics\n        train_key = f'train_{metric_key}'\n        val_key = f'val_{metric_key}'\n        \n        plt.plot(metrics_history['epoch'], metrics_history[train_key], 'b-', label=f'Training {metric_title}')\n        plt.plot(metrics_history['epoch'], metrics_history[val_key], 'r-', label=f'Validation {metric_title}')\n        \n        plt.xlabel('Epoch')\n        plt.ylabel(metric_title)\n        plt.title(f'Training and Validation {metric_title}')\n        plt.legend()\n        plt.grid(True)\n        \n        # Save figure\n        save_path = os.path.join(metrics_dir, f'{metric_key}_plot.png')\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n        plt.close()\n    \n    # Create a combined plot with all metrics\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    axes = axes.flatten()\n    \n    for i, (metric_key, metric_title) in enumerate(metrics_to_plot):\n        train_key = f'train_{metric_key}'\n        val_key = f'val_{metric_key}'\n        \n        axes[i].plot(metrics_history['epoch'], metrics_history[train_key], 'b-', label=f'Training')\n        axes[i].plot(metrics_history['epoch'], metrics_history[val_key], 'r-', label=f'Validation')\n        \n        axes[i].set_xlabel('Epoch')\n        axes[i].set_ylabel(metric_title)\n        axes[i].set_title(metric_title)\n        axes[i].legend()\n        axes[i].grid(True)\n    \n    plt.tight_layout()\n    combined_save_path = os.path.join(metrics_dir, 'combined_metrics_plot.png')\n    plt.savefig(combined_save_path, dpi=300, bbox_inches='tight')\n    plt.show()\n    plt.close()\n    \n    print(f\"Metrics plots saved to {metrics_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T08:12:13.425486Z","iopub.execute_input":"2025-04-26T08:12:13.425725Z","iopub.status.idle":"2025-04-26T08:12:13.434534Z","shell.execute_reply.started":"2025-04-26T08:12:13.425709Z","shell.execute_reply":"2025-04-26T08:12:13.433967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instead of argparse, define these variables directly\nmode = 'train'  # Options: 'train', 'test', 'predict'\ndata_root = '/kaggle/input/crohme-data-basic-final/filtered_basic_arithmetic'\n# /kaggle/input/crohme-data-basic/filtered_basic_arithmetic\ncheckpoint_path = None  # Path to checkpoint if needed\nimage_path = None  # Path to image for prediction\nbatch_size = 32\nnum_epochs = 50\nlearning_rate = 0.001\n\n# Set up configuration\nconfig = Config()\nconfig.data_root = data_root\nconfig.batch_size = batch_size\nconfig.num_epochs = num_epochs\nconfig.learning_rate = learning_rate\nconfig.checkpoint_dir = \"/kaggle/working/checkpoints\"\nconfig.log_dir = \"/kaggle/working/logs\"\n\n# Set seed for reproducibility\nset_seed()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T07:37:54.133705Z","iopub.execute_input":"2025-04-26T07:37:54.133973Z","iopub.status.idle":"2025-04-26T07:37:54.140105Z","shell.execute_reply.started":"2025-04-26T07:37:54.133951Z","shell.execute_reply":"2025-04-26T07:37:54.139239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training mode\nif mode == 'train':\n    if checkpoint_path:\n        # Continue training from checkpoint\n        print(f\"Loading checkpoint from {checkpoint_path}...\")\n        model, tokenizer, loaded_config = load_checkpoint(checkpoint_path)\n        \n        # Update config with loaded config\n        for key, value in vars(loaded_config).items():\n            if key not in ['batch_size', 'num_epochs', 'learning_rate']:\n                setattr(config, key, value)\n        \n        model, tokenizer, metrics_history = train_model(config)\n    else:\n        # Train from scratch\n        model, tokenizer,metrics_history = train_model(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T07:50:57.140434Z","iopub.execute_input":"2025-04-26T07:50:57.140697Z","iopub.status.idle":"2025-04-26T08:10:56.728796Z","shell.execute_reply.started":"2025-04-26T07:50:57.140677Z","shell.execute_reply":"2025-04-26T08:10:56.728142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mode = 'test'  # Options: 'train', 'test', 'predict'\ndata_root = '/kaggle/input/crohme-data-basic-final/filtered_basic_arithmetic'\ncheckpoint_path = \"checkpoints/best_model.pth\"\n# Testing mode\nif mode == 'test':\n    if not checkpoint_path:\n        print(\"Error: Checkpoint path is required for testing\")\n    else:\n        # Load model from checkpoint\n        model, tokenizer, loaded_config = load_checkpoint(checkpoint_path)\n        \n        # Update config with loaded config\n        for key, value in vars(loaded_config).items():\n            setattr(config, key, value)\n        \n        # Test model\n        metrics = test_model(model, tokenizer, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T08:11:09.188469Z","iopub.execute_input":"2025-04-26T08:11:09.188742Z","iopub.status.idle":"2025-04-26T08:11:20.814056Z","shell.execute_reply.started":"2025-04-26T08:11:09.188721Z","shell.execute_reply":"2025-04-26T08:11:20.813343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_dir = \"/kaggle/working\"\nplot_metrics(metrics_history, save_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T08:12:28.577032Z","iopub.execute_input":"2025-04-26T08:12:28.577327Z","iopub.status.idle":"2025-04-26T08:12:32.353754Z","shell.execute_reply.started":"2025-04-26T08:12:28.577305Z","shell.execute_reply":"2025-04-26T08:12:32.352922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mode = 'predict'  # Options: 'train', 'test', 'predict'\ndata_root = '/kaggle/input/crohme-data'\ncheckpoint_path = \"checkpoints/best_model.pth\"\nimage_path = \"/kaggle/input/crohme-data-basic-final/filtered_basic_arithmetic/test/images/expr_000865.png\"\n# Prediction mode\nif mode == 'predict':\n    if not checkpoint_path or not image_path:\n        print(\"Error: Both checkpoint and image paths are required for prediction\")\n    else:\n        # Load model from checkpoint\n        model, tokenizer, loaded_config = load_checkpoint(checkpoint_path)\n        \n        # Update config with loaded config\n        for key, value in vars(loaded_config).items():\n            setattr(config, key, value)\n        filename = os.path.basename(image_path)\n        match = re.search(r'expr_(\\d+)\\.png', filename)\n        id = match.group(1)\n        image_dir = os.path.dirname(image_path)\n        parent_dir = os.path.dirname(image_dir)\n        label_path = os.path.join(parent_dir, 'labels', f'expr_{id}.txt')\n        f = open(label_path,'r')\n        true_latex = f.read()\n        \n        # Predict on image\n        latex = predict_image(model, tokenizer, image_path, config)\n        print(f\"Predicted LaTeX: {latex}\")\n        print(f\"True LaTeX: {true_latex}\")\n        \n        # Optional: Display the image\n        from PIL import Image\n        import matplotlib.pyplot as plt\n        \n        plt.figure(figsize=(10, 4))\n        plt.imshow(Image.open(image_path).convert('L'), cmap='gray')\n        #plt.title(f\"Prediction: {latex}\")\n        plt.axis('off')\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T08:17:05.152915Z","iopub.execute_input":"2025-04-26T08:17:05.15318Z","iopub.status.idle":"2025-04-26T08:17:05.716499Z","shell.execute_reply.started":"2025-04-26T08:17:05.15316Z","shell.execute_reply":"2025-04-26T08:17:05.71594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}